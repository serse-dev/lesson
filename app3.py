import streamlit as st
import pandas as pd
import os
import re
import google.generativeai as genai
from docx import Document
from io import BytesIO

# --- Gemini API Key (—à—É—É–¥ –∫–æ–¥ –¥–æ—Ç–æ—Ä –±–∏—á—Å—ç–Ω) ---
genai.configure(api_key="AIzaSyCB2-tcGlXPEgbc9_jWRH4vwltkm7t15a0")
model = genai.GenerativeModel("gemini-1.5-flash")

# --- –§–∞–π–ª—ã–Ω –∑–∞–º ---
csv_path = "csv_exports/Plan.csv"
shalg_path = "csv_exports/–®–∞–ª–≥—É—É—Ä.csv"
criteria_path = "csv_exports/“Æ—Ä –¥“Ø–Ω–≥–∏–π–Ω —à–∞–ª–≥—É—É—Ä.csv"
level_path = "csv_exports/–ì“Ø–π—Ü—ç—Ç–≥—ç–ª–∏–π–Ω —Ç“Ø–≤—à–∏–Ω.csv"

# --- Streamlit —Ç–æ—Ö–∏—Ä–≥–æ–æ ---
st.set_page_config(page_title="–≠—ç–ª–∂–∏—Ç —Ö–∏—á—ç—ç–ª–∏–π–Ω —Ç”©–ª”©–≤–ª”©–ª—Ç", page_icon="üìö", layout="wide")
st.title("üìö –≠—ç–ª–∂–∏—Ç —Ö–∏—á—ç—ç–ª–∏–π–Ω —Ç”©–ª”©–≤–ª”©–ª—Ç")

def split_sentences(text):
    if not isinstance(text, str):
        return []
    sentences = [s.strip() for s in re.split(r"\.(?!\d)", text) if s.strip()]
    sentences = [s + "." if not s.endswith(".") else s for s in sentences]
    filtered = [s for s in sentences if not re.match(r"^\d+\.\d+\.*$", s)]
    return filtered

# --- –ì–æ–ª —Ö—ç—Å—ç–≥ ---
if not os.path.exists(csv_path):
    st.error(f"{csv_path} —Ñ–∞–π–ª –æ–ª–¥—Å–æ–Ω–≥“Ø–π. Upload —Ö—ç—Å–≥—ç—ç—Ä –æ—Ä—É—É–ª–Ω–∞ —É—É.")
    uploaded = st.file_uploader("CSV —Ñ–∞–π–ª –æ—Ä—É—É–ª–∞—Ö", type=["csv"])
    if uploaded:
        df = pd.read_csv(uploaded)
else:
    df = pd.read_csv(csv_path)

if "df" in locals():
    df = df.rename(columns={
        "–°—É–¥–ª–∞–≥–¥–∞—Ö—É—É–Ω—ã –Ω—ç—Ä": "–•–∏—á—ç—ç–ª–∏–π–Ω –Ω—ç—Ä",
        "–ù—ç–≥–∂–∏–π–Ω –Ω—ç—Ä": "–ù—ç–≥–∂ —Ö–∏—á—ç—ç–ª",
        "–≠—ç–ª–∂–∏—Ç —Ö–∏—á—ç—ç–ª": "–≠—ç–ª–∂–∏—Ç —Ö–∏—á—ç—ç–ª",
        "–≠—ç–ª–∂–∏—Ç —Ö–∏—á—ç—ç–ª–∏–π–Ω –∑–æ—Ä–∏–ª–≥–æ": "–≠—ç–ª–∂–∏—Ç —Ö–∏—á—ç—ç–ª–∏–π–Ω –∑–æ—Ä–∏–ª–≥–æ"
    })

    subjects = df["–•–∏—á—ç—ç–ª–∏–π–Ω –Ω—ç—Ä"].unique()
    subject = st.selectbox("–•–∏—á—ç—ç–ª–∏–π–Ω –Ω—ç—Ä —Å–æ–Ω–≥–æ—Ö", subjects)
    unit_df = df[df["–•–∏—á—ç—ç–ª–∏–π–Ω –Ω—ç—Ä"] == subject]
    units = unit_df["–ù—ç–≥–∂ —Ö–∏—á—ç—ç–ª"].unique()
    unit = st.selectbox("–ù—ç–≥–∂ —Ö–∏—á—ç—ç–ª —Å–æ–Ω–≥–æ—Ö", units)
    lesson_df = unit_df[unit_df["–ù—ç–≥–∂ —Ö–∏—á—ç—ç–ª"] == unit]
    lessons = lesson_df["–≠—ç–ª–∂–∏—Ç —Ö–∏—á—ç—ç–ª"].unique()
    lesson = st.selectbox("–≠—ç–ª–∂–∏—Ç —Ö–∏—á—ç—ç–ª —Å–æ–Ω–≥–æ—Ö", lessons)
    selected = lesson_df[lesson_df["–≠—ç–ª–∂–∏—Ç —Ö–∏—á—ç—ç–ª"] == lesson]

    if selected.empty:
        st.warning("–¢–æ—Ö–∏—Ä–æ—Ö —ç—ç–ª–∂–∏—Ç —Ö–∏—á—ç—ç–ª –æ–ª–¥—Å–æ–Ω–≥“Ø–π.")
    else:
        selected = selected.iloc[0]

        # --- –°—É—Ä–∞–ª—Ü–∞—Ö—É–π–Ω –∑–æ—Ä–∏–ª—Ç ---
        learning_objective = "–•–æ–æ—Å–æ–Ω –±–∞–π–Ω–∞."
        learning_outcome = "–•–æ–æ—Å–æ–Ω –±–∞–π–Ω–∞."
        objectives_list = []
        outcomes_list = []
        if os.path.exists(shalg_path):
            shalg_df = pd.read_csv(shalg_path)
            shalg_match = shalg_df[shalg_df["–≠—ç–ª–∂–∏—Ç —Ö–∏—á—ç—ç–ª"] == lesson]
            if not shalg_match.empty:
                objectives_list = shalg_match["–°—É—Ä–∞–ª—Ü–∞—Ö—É–π–Ω –∑–æ—Ä–∏–ª—Ç"].dropna().unique().tolist()
                outcomes_list = shalg_match["–°—É—Ä–∞–ª—Ü–∞—Ö—É–π–Ω “Ø—Ä –¥“Ø–Ω"].dropna().unique().tolist()
                learning_objective = "\n".join(objectives_list) if objectives_list else "–•–æ–æ—Å–æ–Ω –±–∞–π–Ω–∞."
                learning_outcome = "\n".join(outcomes_list) if outcomes_list else "–•–æ–æ—Å–æ–Ω –±–∞–π–Ω–∞."

        # --- –ú—ç–¥—ç—ç–ª—ç–ª —Ö–∞—Ä—É—É–ª–∞—Ö ---
        st.markdown(f"""
        **–•–∏—á—ç—ç–ª–∏–π–Ω –Ω—ç—Ä:** {selected['–•–∏—á—ç—ç–ª–∏–π–Ω –Ω—ç—Ä']}  
        **–ù—ç–≥–∂ —Ö–∏—á—ç—ç–ª:** {selected['–ù—ç–≥–∂ —Ö–∏—á—ç—ç–ª']}  
        **–≠—ç–ª–∂–∏—Ç —Ö–∏—á—ç—ç–ª:** {selected['–≠—ç–ª–∂–∏—Ç —Ö–∏—á—ç—ç–ª']}  
        **–°—É—Ä–∞–ª—Ü–∞—Ö—É–π–Ω –∑–æ—Ä–∏–ª—Ç:** {learning_objective}  
        **–≠—ç–ª–∂–∏—Ç —Ö–∏—á—ç—ç–ª–∏–π–Ω –∑–æ—Ä–∏–ª–≥–æ:** {selected['–≠—ç–ª–∂–∏—Ç —Ö–∏—á—ç—ç–ª–∏–π–Ω –∑–æ—Ä–∏–ª–≥–æ']}  
        **–°—É—Ä–∞–ª—Ü–∞—Ö—É–π–Ω “Ø—Ä –¥“Ø–Ω:** {learning_outcome}  
        """)

        # --- Prompt “Ø“Ø—Å–≥—ç—Ö ---
        st.markdown("#### üìã Prompt “Ø“Ø—Å–≥—ç—Ö")

        default_prompt = f"""
–¢–∞ –¥–∞—Ä–∞–∞—Ö –º—ç–¥—ç—ç–ª—ç–ª –¥—ç—ç—Ä —Ç—É–ª–≥—É—É—Ä–ª–∞–Ω —ç—ç–ª–∂–∏—Ç —Ö–∏—á—ç—ç–ª–∏–π–Ω –¥—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π —Ç”©–ª”©–≤–ª”©–ª—Ç –≥–∞—Ä–≥–∞:
- –•–∏—á—ç—ç–ª–∏–π–Ω –Ω—ç—Ä: {selected['–•–∏—á—ç—ç–ª–∏–π–Ω –Ω—ç—Ä']}
- –ù—ç–≥–∂ —Ö–∏—á—ç—ç–ª: {selected['–ù—ç–≥–∂ —Ö–∏—á—ç—ç–ª']}
- –≠—ç–ª–∂–∏—Ç —Ö–∏—á—ç—ç–ª: {selected['–≠—ç–ª–∂–∏—Ç —Ö–∏—á—ç—ç–ª']}
- –°—É—Ä–∞–ª—Ü–∞—Ö—É–π–Ω –∑–æ—Ä–∏–ª—Ç: {learning_objective}
- –≠—ç–ª–∂–∏—Ç —Ö–∏—á—ç—ç–ª–∏–π–Ω –∑–æ—Ä–∏–ª–≥–æ: {selected['–≠—ç–ª–∂–∏—Ç —Ö–∏—á—ç—ç–ª–∏–π–Ω –∑–æ—Ä–∏–ª–≥–æ']}
- –°—É—Ä–∞–ª—Ü–∞—Ö—É–π–Ω “Ø—Ä –¥“Ø–Ω: {learning_outcome}

–¢”©–ª”©–≤–ª”©–ª—Ç–∏–π–Ω –∑–∞–≥–≤–∞—Ä:
1. –•–∏—á—ç—ç–ª–∏–π–Ω “Ø–µ —à–∞—Ç (–•—É–≥–∞—Ü–∞–∞, –ë–∞–≥—à–∏–π–Ω “Ø–π–ª –∞–∂–∏–ª–ª–∞–≥–∞–∞, –°—É—Ä–∞–≥—á–∏–π–Ω “Ø–π–ª –∞–∂–∏–ª–ª–∞–≥–∞–∞ —Ö“Ø—Å–Ω—ç–≥—Ç—Ç—ç–π)
2. “Æ–Ω—ç–ª–≥—ç—ç–Ω–∏–π –Ω—ç–≥–∂, —à–∞–ª–≥—É—É—Ä
3. –î–∞—Å–≥–∞–ª, –±–æ–¥–ª–æ–≥–æ, –¥–∞–∞–ª–≥–∞–≤–∞—Ä
4. –î“Ø–≥–Ω—ç–ª—Ç, –≥—ç—Ä–∏–π–Ω –¥–∞–∞–ª–≥–∞–≤–∞—Ä
"""

        extra_text = st.text_area("–ü—Ä–æ–º—Ç–æ–¥ –Ω—ç–º—ç—Ö –∑“Ø–π–ª—ç—ç –±–∏—á–Ω—ç “Ø“Ø", key="extra_prompt")
        prompt_full = default_prompt.strip() + "\n" + extra_text.strip() if extra_text else default_prompt.strip()

        if st.button("‚úçÔ∏è Gemini-—Ä —Ç”©–ª”©–≤–ª”©–≥”©”© –≥–∞—Ä–≥–∞—Ö"):
            with st.spinner("–¢”©–ª”©–≤–ª”©–≥”©”© “Ø“Ø—Å–≥—ç–∂ –±–∞–π–Ω–∞..."):
                response = model.generate_content(prompt_full)
                plan_text = response.text
                st.session_state['ai_plan'] = plan_text

        if st.session_state.get('ai_plan'):
            st.markdown("#### üìù Gemini-–≥—ç—ç—Å –≥–∞—Ä–≥–∞—Å–∞–Ω —Ç”©–ª”©–≤–ª”©–≥”©”©:")
            st.markdown(st.session_state["ai_plan"])

            # --- DOCX –±–æ–ª–≥–æ–∂ —Ö”©—Ä–≤“Ø“Ø–ª—ç—Ö ---
            def generate_docx(text):
                doc = Document()
                doc.add_heading("–≠—ç–ª–∂–∏—Ç —Ö–∏—á—ç—ç–ª–∏–π–Ω —Ç”©–ª”©–≤–ª”©–≥”©”©", 0)

                # Markdown —Ö“Ø—Å–Ω—ç–≥—Ç“Ø“Ø–¥–∏–π–≥ –∏–ª—Ä“Ø“Ø–ª—ç—Ö
                tables = re.findall(r"((?:\|.*\|\n)+)", text)

                # –≠–Ω–≥–∏–π–Ω —Ç–µ–∫—Å—Ç
                for part in text.split("\n"):
                    if "|" in part:
                        continue
                    elif part.strip():
                        doc.add_paragraph(part.strip())

                # –•“Ø—Å–Ω—ç–≥—Ç“Ø“Ø–¥–∏–π–≥ —Ö”©—Ä–≤“Ø“Ø–ª—ç—Ö
                for table in tables:
                    lines = [line.strip() for line in table.strip().split("\n") if line.strip()]
                    headers = [h.strip() for h in lines[0].split("|") if h.strip()]
                    rows = []
                    for line in lines[2:]:
                        cols = [c.strip() for c in line.split("|") if c.strip()]
                        if len(cols) == len(headers):
                            rows.append(cols)

                    if headers and rows:
                        doc_table = doc.add_table(rows=1, cols=len(headers))
                        doc_table.style = "Table Grid"
                        hdr_cells = doc_table.rows[0].cells
                        for i, h in enumerate(headers):
                            hdr_cells[i].text = h
                        for row in rows:
                            row_cells = doc_table.add_row().cells
                            for i, val in enumerate(row):
                                row_cells[i].text = val
                        doc.add_paragraph("")

                buf = BytesIO()
                doc.save(buf)
                buf.seek(0)
                return buf

            docx_file = generate_docx(st.session_state["ai_plan"])
            st.download_button(
                label="‚¨áÔ∏è DOCX —Ç–∞—Ç–∞–∂ –∞–≤–∞—Ö",
                data=docx_file,
                file_name="lesson_plan.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            )
